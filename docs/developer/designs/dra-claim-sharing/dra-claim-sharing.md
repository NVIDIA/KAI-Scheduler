# DRA Claim Sharing

## The Problem

Traditionally, resources in kubernetes were represented by a map of `[name]quntity`, which was essentially a scalar (int/float). Each pod had it's resource request, and each node it's allocatable resources, and if the sum of all resources from all pods is not more than what's allocatable on the node - they could be scheduled on it.  
In KAI, some of these resources would be tracked and accounted for queues: each pod belonged to a podgroup which belonged to a queue. This made it easy to define the resource request and allocation of queues - simply summing up the requests of all pods that belong to that queue.  
The introduction of DRA added many new options for resource consumption; this doc will be scoped to a specific problem - sharing resource claim between queues.

### Sharing resource claims
In DRA, resources are consumed using a `ResourceClaim` object, which defines a request for resources (type, amount, CEL filters etc). Pods then can reference these objects, which allows access to those resources, assuming allocation was successful. A resource claim can be used by multiple pods, and specifically GPU resources, as can be seen in the following [example](https://github.com/jgehrcke/k8s-dra-driver-gpu/blob/main/demo/specs/quickstart/gpu-test3.yaml).  
Even though the claims are namespaced, and can't be shared across namespaces - KAI (by design) doesn't enforce queue-namespace exclusivity, meaning multiple queues can use the same namespace. This means that, theoretically, pods from multiple queues can access the same GPU resource claim, which poses a resource accounting problem: which queue should bear the "cost" of these resources?

## Proposed Solution

In essence, the proposed solution is: 
1. Link each resource claim to a queue, which will account for it, no matter who else uses it
2. Allow queues to opt-out of sharing their resources with others
3. Don't add these requirements for resources that are not accounted in queues

### Details

#### Linking Resource Claims to Queues

The easy way to link resource claims to queues is to use a queue label, like we do for podgroups. A webhook can be used to make this immutable (which we should probably do for podgroups as well, by the way). However, this might make things more difficult for users, and even clash with DRA objects which are created by third parties, or simple examples.  
We could do an implicit linking of objects to queues: DRA objects are bound by the binder, according to the BindRequest spec which is generated by the scheduler, and includes the DRA allocation result. The scheduler can define the queue owner of the claim as the queue of the first pod to bind this ResourceClaim. The binder can then be responsible for adding the label to the claim, to be used by other services. The internal BindRequestInfo in the scheduler can be used to track internally the queue owner of the bind request.

#### Non-accounted resources

It stands to reason that not all DRA resources will be managed by queues: for example, some resources might represent resources which are not in contention and are not meant to be divided by KAI. We can define, either in scheduler shard configuration, or on the device classes, which resources are meant to be accounted by KAI.

### Complications

#### Resource Locking

Imagine queue A creates a claim and lets queue B share it. Queue B allocates a non-preemptible pod on this claim. Queue A's pod is evicted or finishes running, but queue B's non-preemptible pod keeps running, keeping the claim bound and the resources allocated.

Possible solutions:
1. Don't allow foreign queues' non-preemptible pods to share claims
    - What about pods which are protected by min runtime?
    - Could probably be the easiest to implement at first, until we get familiar with more use cases
2. Allow queues to evict claims, even if it will evict foreign pods
    - Need to think how to implement this - maybe an annotation on the claim?

#### Racing 

1. A new `queue` property will be added to the ResourceClaimInfo object
2. On snapshot, if a claim has a queue label, it will populate this property
3. Queue label will be immutable
4. If no queue label exist, the scheduler will consider the first pod that references the claim to be the owner, and will populate this property accordingly
5. The scheduler will explicitly write the queue owner on all bind requests that refer to this claim. This will make sure that things will stay consistent even if the first bind request to be handled is a foreign allocation.
6. On bind, the binder will upsert the queue label to the ResourceClaim object, keeping it consistent during it's lifecycle

#### Orphan Claims

What if a claim is already bound, referenced by multiple queues, but doesn't have a queue reference? Several reasons this could happen: migrations, other schedulers...

Options:
1. Deduct this claim from available resources, and don't attribute it to any queue?
2. Attribute it to the first queue arbitrarily? (alphabetically, pod/job/queue creation time...) 
